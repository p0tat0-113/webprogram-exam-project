<!DOCTYPE html>
<html>
    
<head>
    <link rel="stylesheet" type="text/css" href="style_layout.css">
    <link rel="stylesheet" type="text/css" href="style_color.css">
    <meta author="ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼ 202121028 ê¹€ë™ìš°">
    <meta charset="utf-8"/>
</head>

<body>
    <div class="background">

        <p class="big_title">Data WikiğŸ”</p>

        <div id="left_bar"><!--ì •ë ¬ ì˜µì…˜ì„ ì„ íƒí•˜ëŠ” ë²„íŠ¼ë“¤ ëª¨ì„-->
            <div id="left_bar_innerbox">
            <form>
                <input type="text" class="search1" id="search_window" value="" placeholder="  ì—¬ê¸°ì— ê²€ìƒ‰ì–´ ì…ë ¥" oninput="search()">
                <select class="select1" name="search_option" size="1" id="search_option"><option>name</option><option>task</option></select><br><br>

                <input type="button" class="button5" value="ì´ë¦„ìˆœâ†‘â†“" onclick="sort_id()" ><br><br>

                <input type="button" class="button5-1" value="category ì„ íƒ" onclick="make_button(4)">
                <select class="select1" name="category" size="1" id="category"><option>Image</option><option>Video</option><option>Text</option><option>3-D Image</option><option>Sound</option></select><br><br>

                <input type="button" class="button5" value="taskìˆœâ†‘â†“" onclick="sort_task()"><br><br>

                <input type="button" class="button5-1" value="tutorial O/X ì„ íƒ" onclick="make_button(9)">
                <select class="select1" name="tutorial" size="1" id="tutorial"><option value="yes">O</option><option value="-">X</option></select>         
            </form>
            </div>
        </div>

        <div id="contents">
            <span style="font-size:30px; font-weight: 600; color: grey;">ì—¬ê¸°ì— ë‚´ìš© í‘œì‹œ</span>
        </div><!--ì¢Œì¸¡ ë¦¬ìŠ¤íŠ¸ ë°”ì— ìˆëŠ” ë²„íŠ¼ë“¤ì„ í´ë¦­í•˜ë©´ ì—¬ê¸°ì— ì„¸ë¶€ì •ë³´ê°€ ëœ¸-->

        <iframe src="" name="contents_display" id="contents_display"></iframe><!--contentsë°•ìŠ¤ì— ëœ¨ëŠ” ì„¸ë¶€ì •ë³´ ì¤‘ linkì™€ tutorialë§í¬ë¥¼ ëˆŒë €ì„ë•Œ ì‚¬ìš©ë˜ëŠ” iframe-->

        <div id="left_list_bar"></div><!--ì—¬ê¸°ì— ë²„íŠ¼ë“¤ì´ ë™ì ìœ¼ë¡œ ìƒì„±ë¨-->
    </div>
</body>


    <script> 
    
        function button_to_contents(num){  //ì¢Œì¸¡ ë¦¬ìŠ¤íŠ¸ ë°”ì— ëœ¨ëŠ” ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ contentsë°•ìŠ¤ì— ê·¸ ì„¸ë¶€ì •ë³´ë¥¼ í‘œì‹œì‹œì¼œì£¼ëŠ” í•¨ìˆ˜. ì—¬ê¸°ì„œ numì€ ëª‡ë²ˆì§¸ ì¤„ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì¢Œì¸¡ ë¦¬ìŠ¤íŠ¸ ë°”ì— ëœ¨ëŠ” ë²„íŠ¼ë“¤ì˜ onclick ì†ì„±ì— ì´ í•¨ìˆ˜ê°€ ë“¤ì–´ê°€ìˆìŒ. 
            var contents = document.getElementById("contents");//DOM
            contents.innerHTML = 
            "<span class='contents_box_title'>name: </span>" + dataset[num][1]+"<br>"+
            "<span class='contents_box_title'>category: </span>" + dataset[num][4]+"<br>"+
            "<span class='contents_box_title'>instance: </span>" + dataset[num][7]+"<br>"+
            "<span class='contents_box_title'>task: </span>" + dataset[num][6]+"<br>"+
            "<span class='contents_box_title'>description: </span>" + dataset[num][2]+"<br>"+
            "<span class='contents_box_title'>link: </span><a href='"+dataset[num][3]+"' target='contents_display'>"+dataset[num][3]+"</a>"+
            "&nbsp;<a href='"+dataset[num][3]+"' target='contents_display'><input type='button' value='open page' class='button6'></a><br>"+
            "<span class='contents_box_title'>tutorial: </span><a href='"+dataset[num][9]+"' target='contents_display'>"+dataset[num][9]+"</a>"+
            "&nbsp;<a href='"+dataset[num][9]+"' target='contents_display'><input type='button' value='open page' class='button6'></a>";
            document.getElementById("contents_display").src = dataset[num][3];//í•˜ë‹¨ iframeì— ë§í¬ë¥¼ ë„ì›€
        }

        function make_button(mode){//dataset ë¦¬ìŠ¤íŠ¸ ë²„íŠ¼ì„ ë™ì ìƒì„± í•˜ëŠ” í•¨ìˆ˜
            var length = (dataset.length);//ì²« ì¤„ thë¹¼ê³  60ê°œ
            var list_bar = document.getElementById("left_list_bar");
            var final_code="";//ì´ê²Œ ì¢Œì¸¡ ë¦¬ìŠ¤íŠ¸ ë°•ìŠ¤ì— innerHTMLí•˜ê²Œ ë˜ëŠ” ìµœì¢… ì½”ë“œ

            for(var count=0; count<length; count++){
                if(mode == 0) {//idë¡œ ì •ë ¬
                    var name = dataset[count][1];
                }
                if(mode == 6){//taskë¡œ ì •ë ¬
                    var name = dataset[count][6];
                    if(name == "-") {
                        continue;
                    }
                }
                if(mode == 4){//category ê²€ìƒ‰
                    var name = dataset[count][1];
                    var option = document.getElementById("category")
                    var category = option.options[option.selectedIndex].text;//selectì˜ option í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•

                    var comp = dataset[count][4];//[4]ì¹´í…Œê³ ë¦¬
                    if(comp != category){//í˜„ì¬[count]í–‰ì˜ ì¹´í…Œê³ ë¦¬ê°€ ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì™€ ë‹¤ë¥´ë©´ ë°˜ë³µë¬¸ì„ ì§„í–‰í•˜ì§€ ì•Šê³  ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°
                        continue;
                    }
                }
                if(mode == 9){//íŠœí† ë¦¬ì–¼ ìœ /ë¬´ ê²€ìƒ‰
                    var name = dataset[count][1];
                    var option = document.getElementById("tutorial")
                    var tutorial = option.options[option.selectedIndex].value;//selectì˜ option valueë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•

                    var comp = dataset[count][9];//[9]íŠœí† ë¦¬ì–¼
                    if(tutorial == "-"){ //noë¼ë©´
                        if(comp == "-"){
                            //pass; ìë°”ìŠ¤í¬ë¦½íŠ¸ì—ëŠ” passê°€ ë”°ë¡œ ì—†ë„¤;;
                        }
                        else{
                            continue;
                        }
                    }
                    else{ //yesë¼ë©´
                        if(comp == "-"){
                            continue;
                        }
                        else{
                            //pass;
                        }
                    }
                }
                var code = "<input type='button' class='button6' value='"+name+"' onclick='button_to_contents("+count+")'><br>";
                final_code=final_code+code;
            }
            list_bar.innerHTML=final_code;
        }

        function id_descending(a, b) { // ë‚´ë¦¼ì°¨ìˆœ  
            var numA = (a[0]);
            var numB = (b[0]);  
            return numB < numA ? 1 : -1;
        }
        function id_ascending(a, b) { // ë‚´ë¦¼ì°¨ìˆœ  
            var numA = (a[0]);
            var numB = (b[0]);  
            return numB > numA ? 1 : -1;
        }
        function task_descending(a, b) { // ë‚´ë¦¼ì°¨ìˆœ  
            var numA = (a[6]);
            var numB = (b[6]);  
            return numB < numA ? 1 : -1;
        }
        function task_ascending(a, b) { // ë‚´ë¦¼ì°¨ìˆœ  
            var numA = (a[6]);
            var numB = (b[6]);  
            return numB > numA ? 1 : -1;
        }
  
        var id_bull = 0;
        function sort_id(){//ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒë €ì„ë•Œ ë‚´ë¦¼ì°¨ìˆœ/ì˜¬ë¦¼ì°¨ìˆœì„ ì „í™˜í•˜ê¸° ìœ„í•¨. í˜„ì¬ ì •ë ¬ ë°©ì‹ ìƒíƒœë¥¼ id_bullì— ì €ì¥í•¨
            if(id_bull == 0){
                dataset.sort(id_descending);
                id_bull=1;
            }
            else {
                dataset.sort(id_ascending);
                id_bull=0;
            }
            setTimeout(make_button(0),100);//ì•ˆì •ì ì¸ ë™ì‘ì„ ìœ„í•´ì„œ ì‹œê°„ ì§€ì—° ì¤Œ.
        }
        var task_bull = 0;
        function sort_task(){//ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒë €ì„ë•Œ ë‚´ë¦¼ì°¨ìˆœ/ì˜¬ë¦¼ì°¨ìˆœì„ ì „í™˜í•˜ê¸° ìœ„í•¨. í˜„ì¬ ì •ë ¬ ë°©ì‹ ìƒíƒœë¥¼ task_bullì— ì €ì¥í•¨
            if(task_bull == 0){
                dataset.sort(task_descending);
                task_bull=1;
            }
            else {
                dataset.sort(task_ascending);
                task_bull=0;
            }
            setTimeout(make_button(6),100);
        }

        function search(){//dataset ê²€ìƒ‰ í•¨ìˆ˜
            var input = document.getElementById("search_window");
            console.log(input.value.toUpperCase());

            var option = document.getElementById("search_option")
            var search_option = option.options[option.selectedIndex].text;
            console.log(search_option);

            var length = (dataset.length);//ì²« ì¤„ thë¹¼ê³  60ê°œ
            var list_bar = document.getElementById("left_list_bar");
            var final_code="";//ì´ê²Œ ì¢Œì¸¡ ë¦¬ìŠ¤íŠ¸ ë°•ìŠ¤ì— innerHTMLí•˜ê²Œ ë˜ëŠ” ìµœì¢… ì½”ë“œ

            for(var count=0; count<length; count++){
                if(search_option == "name"){//ê²€ìƒ‰ ì˜µì…˜ ê±°ë¥´ê¸° ê²€ìƒ‰ì˜µì…˜:name
                    var name = dataset[count][1];
                    var comp = dataset[count][0];
                }
                else {//ê²€ìƒ‰ì˜µì…˜:task
                    var name = dataset[count][6];
                    var comp = dataset[count][6];
                }
                comp = comp.toUpperCase();//ì¤‘ìš”! toUpperCase()ë¡œ compì™€ input ë‘˜ë‹¤ ëŒ€ë¬¸ìë¡œ ë§Œë“¤ì–´ì„œ ëŒ€ì†Œë¬¸ì ìƒê´€ì—†ì´ ê²€ìƒ‰ê°€ëŠ¥í•˜ê²Œ í•¨
                if(name == "-") {//taskëª©ë¡ ì¤‘ì—ì„œ "-" ì œê±°
                    continue;
                }

                var result = comp.includes(input.value.toUpperCase());//includeë¥¼ ì´ìš©í•´ì„œ ê° ì…€ì— ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ìì—´ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if(result == true){
                    //pass
                }
                else{
                    continue;
                }

                var code = "<input type='button' class='button6' value='"+name+"' onclick='button_to_contents("+count+")'><br>";
                final_code=final_code+code;
            }
            list_bar.innerHTML=final_code;
        }

            
        setTimeout(sort_id,100);//ì²˜ìŒ í˜ì´ì§€ì— ë“¤ì–´ì™”ì„ë•Œë¶€í„° datasetë¦¬ìŠ¤íŠ¸ë“¤ì´ ë–  ìˆì–´ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ ë•Œë¬¸ì— ë„£ìŒ.

        var dataset = [];
        dataset[0] = ["cmu", "CMU", "-", "http://domedb.perception.cs.cmu.edu/", "Image", "3-D Estimation", "-", "-", "1", "-"];
        dataset[1] = ["human-3.6m", "Human 3.6M", "The Human3.6M dataset is one of the largest motioâ€¦on progressive scan cameras to acquire video ...", "http://vision.imar.ro/human3.6m/description.php", "Image", "3-D Estimation", "3D_Human_Pose_Estimation,3D_Absolute_Human_Pose_Estimation,Human_action_generation", "-", "2", "-"];
        dataset[2] = ["apoloscape", "ApoloScape", "-", "http://apolloscape.auto/", "Image", "Autonomous Driving", "-", "-", "3", "https://capsulesbot.com/blog/2018/08/24/apolloscape-posenet-pytorch.html"];
        dataset[3] = ["cifar-10", "cifar-10", "The CIFAR-10 dataset (Canadian Institute for Advaâ€¦usive classes: airplane, automobile (but not ...", "https://www.cs.toronto.edu/~kriz/cifar.html", "Image", "Classification", "Image_Classification,Image_Generation,Graph_Classification", "60000", "4", "https://ermlab.com/en/blog/nlp/cifar-10-classification-using-keras-tutorial/"];
        dataset[4] = ["cifar-100", "cifar-100", "The CIFAR-100 dataset (Canadian Institute for Advâ€¦20 superclasses. There are 600 images per cla...", "https://www.cs.toronto.edu/~kriz/cifar.html", "Image", "Classification", "Image_Classification,Image_Generation,Few-Shot_Image_Classification", "60000", "5", "-"];
        dataset[5] = ["omniglot", "omniglot", "Omniglot is a large dataset of hand-written charaâ€¦ images and strokes data. Stroke data are coo...", "https://github.com/brendenlake/omniglot#python", "Image", "Classification", "Few-Shot_Image_Classification,Density_Estimation,Multi-Task_Learning", "38300", "6", "https://towardsdatascience.com/few-shot-learning-with-prototypical-networks-87949de03ccd"];
        dataset[6] = ["mnist", "mnist", "The MNIST database (Modified National Institute oâ€¦t is a subset of a larger NIST Special Databa...", "http://yann.lecun.com/exdb/mnist/", "Image", "Classification", "Image_Classification,Image_Generation,Domain_Adaptation", "60000", "7", "https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d"];
        dataset[7] = ["celeba", "celebA", "CelebFaces Attributes dataset contains 202,599 faâ€¦cial attributes like hair color, gender and age.", "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "Image", "Classification", "Image_Classification,Image_Generation,Face_Alignment", "-", "8", "-"];
        dataset[8] = ["svhn", "SVHN", "The Street View House Number (SVHN) is a digit clâ€¦images are centered in the digit of interest,...", "http://ufldl.stanford.edu/housenumbers/", "Image", "Classification", "Image_Classification,Domain_Adaption,Semi-Supervised_Image_Classification", "-", "9", "-"];
        dataset[9] = ["street_style_dataset_of_matzen", "Street Style dataset of Matzen", "-", "http://streetstyle.cs.cornell.edu/", "Image", "Classification", "-", "-", "10", "-"];
        dataset[10] = ["pku_vehicleid", "PKU VehicleID (VehicleID)", "The â€œVehicleIDâ€ dataset contains CARS captured duâ€¦e entire dataset. Each image is attached with...", "https://pkuml.org/resources/pku-vehicleid.html", "Image", "Classification", "Vehicle_Re-Identification", "-", "11", "-"];
        dataset[11] = ["the_in-shop_clothes", "The In-shop Clothes", "-", "http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html", "Image", "Classification", "-", "-", "12", "-"];
        dataset[12] = ["taskonomy", "Taskonomy", "Taskonomy provides a large and high-quality datasâ€¦, and MIT Places. &nbsp;Globally consistent c...", "http://taskonomy.stanford.edu/", "Image", "Depth Estimation", "Depth_Estimation,Surface_Normals_Estimation", "-", "13", "-"];
        dataset[13] = ["cuhk_face_sketch_database", "CUHK Face Sketch Database (CUFS)", "-", "http://www.ee.cuhk.edu.hk/~xgwang/datasets.html", "Image", "Face Sketch", "-", "-", "14", "-"];
        dataset[14] = ["chestx-ray8", "ChestX-ray8", "ChestX-ray8 is a medical imaging dataset which coâ€¦ined from the text radiological reports via N...", "https://www.kaggle.com/nih-chest-xrays/data", "Image", "Medical Classification", "Image_Classification,Computed_Tomography(CT)", "-", "15", "-"];
        dataset[15] = ["kitti", "kitti", "KITTI (Karlsruhe Institute of Technology and Toyoâ€¦orded with a variety of sensor modalities, in...", "http://www.cvlibs.net/datasets/kitti/", "Image", "Object Detection", "Object_Detection,Semantice_Segmentation,Image_Super-Resolution", "&gt;100 GB of data", "16", "https://github.com/joseph-zhang/KITTI-TorchLoader"];
        dataset[16] = ["pascal_voc_2012", "pascal voc 2012", "-", "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/", "Image", "Object Detection", "-", "-", "17", "-"];
        dataset[17] = ["cityscapes", "Cityscapes", "Cityscapes is a large-scale database which focuseâ€¦lat surfaces, humans, vehicles, constructions...", "https://www.cityscapes-dataset.com/", "Image", "Object Detection", "Image_Generation,Semantic_Segmentation,Image-to-Image_Translation", "25000", "18", "-"];
        dataset[18] = ["aflw", "AFLW", "The Annotated Facial Landmarks in the Wild (AFLW)â€¦nder) as well as general imaging and environm...", "https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/", "Image", "Object Detection", "Face_Alignment,Facial_Landmark's_Detection,Low-Light_Image_Enhancement", "-", "19", "-"];
        dataset[19] = ["caltech_101", "Caltech 101", "The Caltech101 dataset contains images from 101 oâ€¦ries. For each object category, there are abo...", "http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/", "Image", "Object Detection", "Fine-Grained_Image_Classification,Semi-Supervised_Image_Classificatino,Density_Estimation", "9146", "20", "-"];
        dataset[20] = ["caltech_256", "Caltech 256", "Caltech-256 is an object recognition dataset contâ€¦least 80 images. The dataset is a superset of...", "https://authors.library.caltech.edu/7694/", "Image", "Object Detection", "Few-Shot_Image_Classification,Semi-Supervised_Image_Classification", "30607", "21", "-"];
        dataset[21] = ["amazon", "Amazon", "-", "https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cd-create-dataset.html", "Image", "Object Detection", "-", "-", "22", "-"];
        dataset[22] = ["nlpr", "NLPR", "The NLPR dataset for salient object detection conâ€¦., offices, campuses, streets and supermarkets).", "https://www.abbreviationfinder.org/ko/acronyms/nlpr.html", "Image", "Object Detection", "RGB-D_Salient_Object_Detection", "-", "23", "-"];
        dataset[23] = ["coco", "coco", "The MS COCO (Microsoft Common Objects in Context)â€¦ version of MS COCO dataset was released in 2...", "https://cocodataset.org/#home", "Image", "Object Recognition", "Pose_Estimation,Object_Detection,Semantic_Segmentation", "2500000", "24", "https://medium.com/fullstackai/how-to-train-an-obâ€¦ith-your-own-coco-dataset-in-pytorch-319e7090da5"];
        dataset[24] = ["imagenet", "imagenet", "The ImageNet dataset contains 14,197,122 annotateâ€¦age classification and object detection. The ...", "https://www.image-net.org/", "Image", "Object Recognition", "Image_Classification,Image_Generation,Few-Shot_Learning", "14197122", "25", "-"];
        dataset[25] = ["sun", "sun", "-", "https://vision.princeton.edu/projects/2010/SUN/", "Image", "Object Recognition", "-", "131,067", "26", "-"];
        dataset[26] = ["lsun", "lsun", "The Large-scale Scene Understanding (LSUN) challeâ€¦such as dining room, bedroom, chicken, outdoo...", "https://www.yf.io/p/lsun", "Image", "Saliency Detection", "Image_Generation", "-", "27", "-"];
        dataset[27] = ["replica", "Replica", "The Replica Dataset is a dataset of high quality â€¦urface information, planar segmentation as we...", "https://github.com/facebookresearch/Replica-Dataset", "Image", "Scene Generation", "Domain_Adaption,Visual_Navigation,Scene_Generation", "-", "28", "-"];
        dataset[28] = ["scannet", "scannet", "ScanNet is an instance-level indoor RGB-D datasetâ€¦s collected 1513 annotated scans with an appr...", "http://www.scan-net.org/", "Image", "Semantic Segmentation", "Semantic_Segmentation,Depth_Estimation,3D_Reconstruction", "-", "29", "-"];
        dataset[29] = ["nyu_depth_v1_v2", "nyu depth V1, V2", "-", "https://cs.nyu.edu", "Image", "Semantic Segmentation", "-", "-", "30", "-"];
        dataset[30] = ["lip", "lip", "The LIP (Look into Person) dataset is a large-scaâ€¦2D human poses with 16 key points. The images...", "http://sysu-hcp.net/lip/index.php", "Image", "Semantic Segmentation", "Semantic_Segmentation", "-", "31", "-"];
        dataset[31] = ["ade", "ADE", "The ADE20K semantic segmentation dataset containsâ€¦clude stuffs like sky, road, grass, and discr...", "https://groups.csail.mit.edu/vision/datasets/ADE20K/index.html", "Image", "Semantic Segmentation", "Semantic_Segmentation,Image-to-Image_Translation,Scene_Understanding", "-", "32", "-"];
        dataset[32] = ["ffhq", "ffhq", "Flickr-Faces-HQ (FFHQ) consists of 70,000 high-quâ€¦ssories such as eyeglasses, sunglasses, hats,...", "https://github.com/NVlabs/ffhq-dataset", "Image", "Super Resolution", "Image_Generation,Image_Super-Resolution,Image_Inpainting", "-", "33", "-"];
        dataset[33] = ["ucf", "ucf", "UCF101 dataset is an extension of UCF50 and consiâ€¦ Human-object interactions, Playing musical i...", "https://www.crcv.ucf.edu/data/UCF101.php#Results_on_UCF101", "Video", "Action Recognition", "Temporal_Action_Localization,Action_Recognition,Action_Detection", "-", "1", "-"];
        dataset[34] = ["activitynet", "Activitynet", "The ActivityNet dataset contains 200 different tyâ€¦ms of both the number of activity categories ...", "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "Video", "Action Recognition", "Temporal_Action_Localization,Action_Recognition,Action_Classification", "-", "2", "-"];
        dataset[35] = ["ntu", "ntu", "-", "http://rose1.ntu.edu.sg/datasets/actionrecognition.asp", "Video", "Action Recognition", "-", "-", "3", "-"];
        dataset[36] = ["kinetics", "kinetics", "The Kinetics dataset is a large-scale, high-qualiâ€¦clips for each action class. Each video clip ...", "https://arxiv.org/abs/1705.06950", "Video", "Action Recognition", "Temporal_Action_Localization,Video_Classification,Action_Recognition", "-", "4", "-"];
        dataset[37] = ["youtube_8m_segments_dataset", "YouTube-8M Segments Dataset", "The YouTube-8M dataset is a large scale video datâ€¦set, and test set. In the training set, each ...", "http://research.google.com/youtube8m/index.html", "Video", "Classification", "Video_Classification,Video_Prediction", "8 million", "5", "-"];
        dataset[38] = ["davis_16", "davis 16", "DAVIS16 is a dataset for video object segmentatioâ€¦). Per-frame pixel-wise annotations are offered.", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "Video_Object_Segmentation,Video_Salient_Object_Detection,Unsupervised_Video_Object_Segmentation", "-", "6", "-"];
        dataset[39] = ["davis_17", "davis 17", "DAVIS17 is a dataset for video object segmentatioâ€¦ for training, 30 for validation, 60 for testing", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "Semantic_Segmentation,Video_Object_Segmentation,Referring_Expression_Segmentation", "-", "7", "-"];
        dataset[40] = ["davis_18", "davis 18", "-", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "-", "-", "8", "-"];
        dataset[41] = ["davis_19", "davis 19", "-", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "-", "-", "9", "-"];
        dataset[42] = ["mot", "MOT", "-", "https://motchallenge.net/", "Video", "Object Tracking", "-", "-", "10", "-"];
        dataset[43] = ["vot", "vot", "-", "https://www.votchallenge.net/index.html", "Video", "Object Tracking", "-", "-", "11", "-"];
        dataset[44] = ["dexter", "dexter", "-", "http://archive.ics.uci.edu/ml//datasets/Dexter", "Text", "Classification", "-", "2600", "1", "-"];
        dataset[45] = ["ubuntu_dialogue", "ubuntu dialogue", "Ubuntu Dialogue Corpus (UDC) is a dataset containâ€¦ilding dialogue managers based on neural lang...", "https://ubuntudialogue.org/", "Text", "Dialogue Generation", "Dialogue_Generation,Conversational_Response_Selection,Answer_Selection", "-", "2", "-"];
        dataset[46] = ["wmt19", "wmt19", "-", "http://www.statmt.org/wmt19/", "Text", "Machine Translation", "-", "-", "3", "-"];
        dataset[47] = ["wmt18", "wmt18", "WMT 2018 is a collection of datasets used in sharâ€¦on. &nbsp;&nbsp;&nbsp;The conference featured...", "http://www.statmt.org/wmt18/papers.html", "Text", "Machine Translation", "Machine_Translation", "-", "4", "-"];
        dataset[48] = ["wmt17", "wmt17", "-", "http://www.statmt.org/wmt17/results.html", "Text", "Machine Translation", "-", "-", "5", "-"];
        dataset[49] = ["wmt16", "wmt16", "WMT 2016 is a collection of datasets used in sharâ€¦red tasks: &nbsp;&nbsp;&nbsp;a news translati...", "http://www.statmt.org/wmt16/", "Text", "Machine Translation", "Machine_Translation,Unsupervised_Machine_Translation", "-", "6", "-"];
        dataset[50] = ["wmt15", "wmt15", "WMT 2015 is a collection of datasets used in sharâ€¦quality estimation task, &nbsp;an automatic p...", "http://www.statmt.org/wmt15/", "Text", "Machine Translation", "Machine_Translation", "-", "7", "-"];
        dataset[51] = ["wmt14", "wmt14", "WMT 2014 is a collection of datasets used in sharâ€¦ics task, &nbsp;a medical text translation task.", "http://www.statmt.org/wmt14/", "Text", "Machine Translation", "Machine_Translation,Unsupervised_Machine_Translation", "-", "8", "-"];
        dataset[52] = ["semeval-2016", "SemEval-2016", "-", "https://alt.qcri.org/semeval2016/index.php?id=tasks", "Text", "Word Sentiment", "-", "-", "9", "-"];
        dataset[53] = ["bfm", "BFM", "-", "https://faces.dmi.unibas.ch/bfm/?nav=1-0&amp;id=basel_face_model", "3-D Image", "3-D Estimation", "-", "-", "1", "-"];
        dataset[54] = ["pix3d", "Pix3D", "The Pix3D dataset is a large-scale benchmark of dâ€¦struction, retrieval, viewpoint estimation, etc.", "http://pix3d.csail.mit.edu/", "3-D Image", "Classification", "3D_Shape_Reconstruction,3D_Shape_Modeling,3D_Shape_Classification", "-", "2", "-"];
        dataset[55] = ["shrec", "shrec", "The SHREC dataset contains 14 dynamic gestures peâ€¦ and 10 times by each participant in two way:...", "http://tosca.cs.technion.ac.il/book/shrec_robustness2010.html", "3-D Image", "Object Recognition", "Gesture_Recognition,Hand_Gesture_Recognition,Skeleton_Based_Action_Recognition", "-", "3", "-"];
        dataset[56] = ["shapenetcore", "shapenetCore", "-", "https://www.shapenet.org/", "3-D Image", "Semantic Segmentation", "-", "-", "4", "-"];
        dataset[57] = ["faust", "faust", "The FAUST dataset is a dataset of real 3D scans oâ€¦100 non-watertight meshes with 6,890 nodes each.", "http://faust.is.tue.mpg.de/", "3-D Image", "Semantic Segmentation", "Semantic_Segmentation,3D_Reconstruction,3D_Point_Cloud_Matching", "-", "5", "-"];
        dataset[58] = ["scape", "Scape", "-", "https://ai.stanford.edu/~drago/Projects/scape/scape.html", "3-D Image", "3-D Estimation", "-", "-", "6", "-"];
        dataset[59] = ["voxceleb", "VoxCeleb", "-", "http://www.robots.ox.ac.uk/~vgg/data/voxceleb/", "Sound", "Video Reconstruction", "-", "-", "1", "-"];
    </script>
</html>

