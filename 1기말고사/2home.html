<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" type="text/css" href="style_layout.css">
    <link rel="stylesheet" type="text/css" href="style_color.css">
    <meta author="ÏÜåÌîÑÌä∏Ïõ®Ïñ¥ÌïôÍ≥º 202121028 ÍπÄÎèôÏö∞">
    <meta charset="utf-8"/>
</head>

<body>
<div class="background">
    <div class="windowcon"><!--ÏÇ¨ÏßÑ 5Ïû• Î¨¥Ìïú Ïä¨ÎùºÏù¥Îìú-->
        <div class="container">
            <img src="images/01.png" width="1280px" height="720">
            <img src="images/02.png" width="1280px" height="720">
            <img src="images/03.png" width="1280px" height="720">
            <img src="images/04.png" width="1280px" height="720">
            <img src="images/05.png" width="1280px" height="720">
            <img src="images/01.png" width="1280px" height="720"><!--Ï≤òÏùå ÏÇ¨ÏßÑÏùÑ ÎÅùÏóê Íº≠ ÎÑ£Ïñ¥Ï§òÏïºÌï®-->       
        </div>
    </div>

    <div id="left_box"><!--ÏôºÏ™Ω Í≥µÏßÄÏÇ¨Ìï≠ ÏïåÎ¶ºÌåê-->
        <p id="box_title">Announcementüì¢</p>
        <ol id="box_list">
            <div id="list1">
            
            </div>
        </ol>
    </div>

    <div id="right_box"><!--Ïò§Î•∏Ï™Ω Ïù∏Í∏∞Í∏Ä ÏïåÎ¶ºÌåê-->
        <p id="box_title">Hotüî•</p>
        <ol id="box_list">
            <div id="list2">
            
            </div>
        </ol>
    </div>
</div>
</body>

    <script>
    const windowcon = document.querySelector('.windowcon'),
        container = document.querySelector('.container'),
            slides = document.querySelectorAll('img'),
            slidecounter = slides.length;
    let currentIndex = 0;
    
    for(i=0; i < slidecounter; i++){
        slides[i].style.left = `${i*100}%`;
    
    }
    
    function calcul(){
        for(i=0; i<slidecounter; i++){
            if(windowcon.offsetHeight < slides[i].offsetHeight){
                windowcon.style.height = slides[i].offsetHeight +"px";
                windowcon.style.width = slides[i].offsetWidth +"px";
            }
        }
    }
    
    calcul();
    var lele = 0;
    var i = 0;
    
    function moveevent(){
        setInterval(function(){
            lele += 100;
            container.style.transition ='.3s'
            container.style.left ="-" + lele +"%";
            i++;
            
            if(i === slidecounter-1){
                setTimeout(function(){
                    container.style.transition ='0s'
                    lele = 0;
                    container.style.left ="-" + lele +"%";
                },201)
                i = 0;
            }
        }, 5000)//5000msÎßàÎã§ ÏÇ¨ÏßÑÏùÑ ÎÑòÍπÄ
    }

    moveevent();

    function config(){
        if(!window.localStorage.getItem("board")){//localstorageÏóê boardÍ∞Ä ÏóÜÏñ¥Ïïº ÏûëÎèô
            console.log("board set!");
            window.localStorage.setItem("board",JSON.stringify(board1));//localstorageÏóê Î∞∞Ïó¥ÏùÑ Ï†ÄÏû•ÌïòÎäî Î∞©Î≤ï
            //Ï≤´ Ïã§ÌñâÏãú localstorageÏóê boardÎ•º Î∞∞Ïó¥ ÌòïÌÉúÎ°ú Ï†ÄÏû•Ìï®.
            //Ïó¨Í∏∞ÏóêÏÑú forumÍ∏Ä Îú®Îäî Í≤ÉÍ≥º forumÏùò Í∏Ä ÏûëÏÑ±, ÏàòÏ†ï, ÏÇ≠Ï†ú, Ï¢ãÏïÑÏöî Í∏∞Îä•Ïù¥ Ïó∞ÎèôÎêòÍ≤å ÌïòÍ∏∞ ÏúÑÌï®
        }
    }

    var board1 = [];
    board1[0] = ["ÏïàÎÖïÌïòÏÑ∏Ïöî","DatawikiÏóê Ïò§Ïã† Í≤ÉÏùÑ ÌôòÏòÅÌï©ÎãàÎã§.","2022.05.01","10"];
    board1[1] = ["Í¥ÄÎ¶¨ÏûêÏÜåÍ∞ú","DatawikiÏùò Í¥ÄÎ¶¨ÏûêÎ•º ÏÜåÍ∞ú Ìï©ÎãàÎã§.","2022.05.05","5"];
    board1[2] = ["ÏµúÏã† Îâ¥Ïä§","DatawikiÎäî ÏÉÅÎ™ÖÎåÄ Ï≤úÏïàÏ∫†ÌçºÏä§ÏóêÏÑú Ï£ºÍ¥ÄÌïòÎäî Îç∞Ïù¥ÌÑ∞ Í≥µÏú† ÏÇ¨Ïù¥Ìä∏ ÏûÖÎãàÎã§.","2022.05.06","22"];
    board1[3] = ["Í≥µÏßÄÏÇ¨Ìï≠","DatawikiÍ∞Ä ÏÉàÎ°≠Í≤å Îã®Ïû•Îê©ÎãàÎã§. ÎçîÏö± Í∞úÏÑ†Îêú ÏÇ¨Ïù¥Ìä∏Î•º Í∏∞ÎåÄÌï¥ Ï£ºÏÑ∏Ïöî.","2022.05.08","100"];
    board1[4] = ["Ï∂îÍ∞Ä Í≥µÏßÄ ÏÇ¨Ìï≠","DatawikiÎ•º ÏÉàÎ°≠Í≤å Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌïú Î©îÎãàÏ†ÄÎ•º Í≥µÎ™®Ìï©ÎãàÎã§. ","2022.05.10","1"];
    board1[5] = ["2022ÎÖÑ Îç∞Ïù¥ÌÑ∞ÏúÑÌÇ§ Í≥µÎ™®Ï†Ñ","2022ÎÖÑ Îç∞Ïù¥ÌÑ∞ ÏúÑÌÇ§Îäî Ïù¥ÎØ∏ÏßÄ Ïù∏Ïãù Ï£ºÏ†úÎ°ú ÏßÑÌñâÌï©ÎãàÎã§.","2022.05.12","44"];
    board1[6] = ["Í≥µÎ™®Ï†Ñ Ï¢ÖÎ£å ÏïàÎÇ¥","2022ÎÖÑ Îç∞Ïù¥ÌÑ∞ ÏúÑÌÇ§ Í≥µÎ™®Ï†ÑÏù¥ ÏÑ±Í≥µÎ¶¨Ïóê ÎßàÍ∞ê ÎêòÏóàÏäµÎãàÎã§. ","2022.05.14","50"];
    board1[7] = ["2023ÎÖÑ Îç∞Ïù¥ÌÑ∞ ÏúÑÌÇ§ Í≥µÎ™®Ï†Ñ","2023ÎÖÑ Îç∞Ïù¥ÌÑ∞ ÏúÑÌÇ§ Í≥µÎ™®Ï†ÑÏùÑ ÏÜåÍ∞ú Ìï©ÎãàÎã§.","2022.05.16","49"];
    board1[8] = ["ÌïôÍ∏∞ Ï¢ÖÎ£å ÏïàÎÇ¥","22ÎÖÑ 1ÌïôÍ∏∞ Ï¢ÖÎ£åÍπåÏßÄ Ïù¥Ï†ú 6Ï£º ÎÇ®ÏïòÏäµÎãàÎã§. ","2022.05.18","17"];
    board1[9] = ["ÌäúÌÜ†Î¶¨Ïñº Í≥µÏßÄ","tensorflowÏôÄ kerasÏùò ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Ïù¥Ïö©ÌïòÏó¨ Í∞ôÏù¥ Í≥µÎ∂ÄÌï¥ Î¥ÖÏãúÎã§.","2022.05.19","84"];
    board1[10] = ["Ïã†Í∑ú Îç∞Ïù¥ÌÑ∞ ÏöîÏ≤≠","ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌïòÏã† Î∂ÑÏùÄ Ïñ∏Ï†úÎì† ÏöîÏ≤≠ Î∞îÎûçÎãàÎã§.","2022.05.21","3"];
    board1[11] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[12] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[13] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[14] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[15] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[16] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[17] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[18] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[19] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[20] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[21] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[22] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[23] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[24] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[25] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[26] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[27] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[28] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[29] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[30] = ["ÌéòÏù¥ÏßÄ ÎÑòÍπÄ Í∏∞Îä• ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏÉùÏÑ±Ìïú Í∏Ä","ÌÖåÏä§Ìä∏","1999.12.31","0"];
    board1[31] = ["< Î≤ÑÌäºÏùÑ ÎàÑÎ•¥Î©¥ Îã§Ïùå ÌéòÏù¥ÏßÄÎ°ú ÎÑòÏñ¥Í∞ê","ÌÖåÏä§Ìä∏","2000.01.01","0"];
    board1[32] = ["> Î≤ÑÌäºÏùÑ ÎàÑÎ•¥Î©¥ Ïù¥Ï†Ñ ÌéòÏù¥ÏßÄÎ°ú ÎÑòÏñ¥Í∞ê","ÌÖåÏä§Ìä∏","2000.01.01","0"];
    board1[33] = ["Í¥ÄÎ¶¨Ïûê ÎπÑÎ∞ÄÎ≤àÌò∏Îäî 1234 ÏûÖÎãàÎã§.","ÏïÑÏãúÍ≤†Ï£µ?","2001.01.01","0"]

    var dataset = [];
    dataset[0] = ["cmu", "CMU", "-", "http://domedb.perception.cs.cmu.edu/", "Image", "3-D Estimation", "-", "-", "1", "-"];
    dataset[1] = ["human-3.6m", "Human 3.6M", "The Human3.6M dataset is one of the largest motio‚Ä¶on progressive scan cameras to acquire video ...", "http://vision.imar.ro/human3.6m/description.php", "Image", "3-D Estimation", "3D_Human_Pose_Estimation,3D_Absolute_Human_Pose_Estimation,Human_action_generation", "-", "2", "-"];
    dataset[2] = ["apoloscape", "ApoloScape", "-", "http://apolloscape.auto/", "Image", "Autonomous Driving", "-", "-", "3", "https://capsulesbot.com/blog/2018/08/24/apolloscape-posenet-pytorch.html"];
    dataset[3] = ["cifar-10", "cifar-10", "The CIFAR-10 dataset (Canadian Institute for Adva‚Ä¶usive classes: airplane, automobile (but not ...", "https://www.cs.toronto.edu/~kriz/cifar.html", "Image", "Classification", "Image_Classification,Image_Generation,Graph_Classification", "60000", "4", "https://ermlab.com/en/blog/nlp/cifar-10-classification-using-keras-tutorial/"];
    dataset[4] = ["cifar-100", "cifar-100", "The CIFAR-100 dataset (Canadian Institute for Adv‚Ä¶20 superclasses. There are 600 images per cla...", "https://www.cs.toronto.edu/~kriz/cifar.html", "Image", "Classification", "Image_Classification,Image_Generation,Few-Shot_Image_Classification", "60000", "5", "-"];
    dataset[5] = ["omniglot", "omniglot", "Omniglot is a large dataset of hand-written chara‚Ä¶ images and strokes data. Stroke data are coo...", "https://github.com/brendenlake/omniglot#python", "Image", "Classification", "Few-Shot_Image_Classification,Density_Estimation,Multi-Task_Learning", "38300", "6", "https://towardsdatascience.com/few-shot-learning-with-prototypical-networks-87949de03ccd"];
    dataset[6] = ["mnist", "mnist", "The MNIST database (Modified National Institute o‚Ä¶t is a subset of a larger NIST Special Databa...", "http://yann.lecun.com/exdb/mnist/", "Image", "Classification", "Image_Classification,Image_Generation,Domain_Adaptation", "60000", "7", "https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d"];
    dataset[7] = ["celeba", "celebA", "CelebFaces Attributes dataset contains 202,599 fa‚Ä¶cial attributes like hair color, gender and age.", "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "Image", "Classification", "Image_Classification,Image_Generation,Face_Alignment", "-", "8", "-"];
    dataset[8] = ["svhn", "SVHN", "The Street View House Number (SVHN) is a digit cl‚Ä¶images are centered in the digit of interest,...", "http://ufldl.stanford.edu/housenumbers/", "Image", "Classification", "Image_Classification,Domain_Adaption,Semi-Supervised_Image_Classification", "-", "9", "-"];
    dataset[9] = ["street_style_dataset_of_matzen", "Street Style dataset of Matzen", "-", "http://streetstyle.cs.cornell.edu/", "Image", "Classification", "-", "-", "10", "-"];
    dataset[10] = ["pku_vehicleid", "PKU VehicleID (VehicleID)", "The ‚ÄúVehicleID‚Äù dataset contains CARS captured du‚Ä¶e entire dataset. Each image is attached with...", "https://pkuml.org/resources/pku-vehicleid.html", "Image", "Classification", "Vehicle_Re-Identification", "-", "11", "-"];
    dataset[11] = ["the_in-shop_clothes", "The In-shop Clothes", "-", "http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html", "Image", "Classification", "-", "-", "12", "-"];
    dataset[12] = ["taskonomy", "Taskonomy", "Taskonomy provides a large and high-quality datas‚Ä¶, and MIT Places. &nbsp;Globally consistent c...", "http://taskonomy.stanford.edu/", "Image", "Depth Estimation", "Depth_Estimation,Surface_Normals_Estimation", "-", "13", "-"];
    dataset[13] = ["cuhk_face_sketch_database", "CUHK Face Sketch Database (CUFS)", "-", "http://www.ee.cuhk.edu.hk/~xgwang/datasets.html", "Image", "Face Sketch", "-", "-", "14", "-"];
    dataset[14] = ["chestx-ray8", "ChestX-ray8", "ChestX-ray8 is a medical imaging dataset which co‚Ä¶ined from the text radiological reports via N...", "https://www.kaggle.com/nih-chest-xrays/data", "Image", "Medical Classification", "Image_Classification,Computed_Tomography(CT)", "-", "15", "-"];
    dataset[15] = ["kitti", "kitti", "KITTI (Karlsruhe Institute of Technology and Toyo‚Ä¶orded with a variety of sensor modalities, in...", "http://www.cvlibs.net/datasets/kitti/", "Image", "Object Detection", "Object_Detection,Semantice_Segmentation,Image_Super-Resolution", "&gt;100 GB of data", "16", "https://github.com/joseph-zhang/KITTI-TorchLoader"];
    dataset[16] = ["pascal_voc_2012", "pascal voc 2012", "-", "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/", "Image", "Object Detection", "-", "-", "17", "-"];
    dataset[17] = ["cityscapes", "Cityscapes", "Cityscapes is a large-scale database which focuse‚Ä¶lat surfaces, humans, vehicles, constructions...", "https://www.cityscapes-dataset.com/", "Image", "Object Detection", "Image_Generation,Semantic_Segmentation,Image-to-Image_Translation", "25000", "18", "-"];
    dataset[18] = ["aflw", "AFLW", "The Annotated Facial Landmarks in the Wild (AFLW)‚Ä¶nder) as well as general imaging and environm...", "https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/", "Image", "Object Detection", "Face_Alignment,Facial_Landmark's_Detection,Low-Light_Image_Enhancement", "-", "19", "-"];
    dataset[19] = ["caltech_101", "Caltech 101", "The Caltech101 dataset contains images from 101 o‚Ä¶ries. For each object category, there are abo...", "http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/", "Image", "Object Detection", "Fine-Grained_Image_Classification,Semi-Supervised_Image_Classificatino,Density_Estimation", "9146", "20", "-"];
    dataset[20] = ["caltech_256", "Caltech 256", "Caltech-256 is an object recognition dataset cont‚Ä¶least 80 images. The dataset is a superset of...", "https://authors.library.caltech.edu/7694/", "Image", "Object Detection", "Few-Shot_Image_Classification,Semi-Supervised_Image_Classification", "30607", "21", "-"];
    dataset[21] = ["amazon", "Amazon", "-", "https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cd-create-dataset.html", "Image", "Object Detection", "-", "-", "22", "-"];
    dataset[22] = ["nlpr", "NLPR", "The NLPR dataset for salient object detection con‚Ä¶., offices, campuses, streets and supermarkets).", "https://www.abbreviationfinder.org/ko/acronyms/nlpr.html", "Image", "Object Detection", "RGB-D_Salient_Object_Detection", "-", "23", "-"];
    dataset[23] = ["coco", "coco", "The MS COCO (Microsoft Common Objects in Context)‚Ä¶ version of MS COCO dataset was released in 2...", "https://cocodataset.org/#home", "Image", "Object Recognition", "Pose_Estimation,Object_Detection,Semantic_Segmentation", "2500000", "24", "https://medium.com/fullstackai/how-to-train-an-ob‚Ä¶ith-your-own-coco-dataset-in-pytorch-319e7090da5"];
    dataset[24] = ["imagenet", "imagenet", "The ImageNet dataset contains 14,197,122 annotate‚Ä¶age classification and object detection. The ...", "http://image-net.org/about-overview", "Image", "Object Recognition", "Image_Classification,Image_Generation,Few-Shot_Learning", "14197122", "25", "-"];
    dataset[25] = ["sun", "sun", "-", "https://vision.princeton.edu/projects/2010/SUN/", "Image", "Object Recognition", "-", "131,067", "26", "-"];
    dataset[26] = ["lsun", "lsun", "The Large-scale Scene Understanding (LSUN) challe‚Ä¶such as dining room, bedroom, chicken, outdoo...", "https://www.yf.io/p/lsun", "Image", "Saliency Detection", "Image_Generation", "-", "27", "-"];
    dataset[27] = ["replica", "Replica", "The Replica Dataset is a dataset of high quality ‚Ä¶urface information, planar segmentation as we...", "https://github.com/facebookresearch/Replica-Dataset", "Image", "Scene Generation", "Domain_Adaption,Visual_Navigation,Scene_Generation", "-", "28", "-"];
    dataset[28] = ["scannet", "scannet", "ScanNet is an instance-level indoor RGB-D dataset‚Ä¶s collected 1513 annotated scans with an appr...", "http://www.scan-net.org/", "Image", "Semantic Segmentation", "Semantic_Segmentation,Depth_Estimation,3D_Reconstruction", "-", "29", "-"];
    dataset[29] = ["nyu_depth_v1_v2", "nyu depth V1, V2", "-", "https://cs.nyu.edu", "Image", "Semantic Segmentation", "-", "-", "30", "-"];
    dataset[30] = ["lip", "lip", "The LIP (Look into Person) dataset is a large-sca‚Ä¶2D human poses with 16 key points. The images...", "http://sysu-hcp.net/lip/index.php", "Image", "Semantic Segmentation", "Semantic_Segmentation", "-", "31", "-"];
    dataset[31] = ["ade", "ADE", "The ADE20K semantic segmentation dataset contains‚Ä¶clude stuffs like sky, road, grass, and discr...", "https://groups.csail.mit.edu/vision/datasets/ADE20K/index.html", "Image", "Semantic Segmentation", "Semantic_Segmentation,Image-to-Image_Translation,Scene_Understanding", "-", "32", "-"];
    dataset[32] = ["ffhq", "ffhq", "Flickr-Faces-HQ (FFHQ) consists of 70,000 high-qu‚Ä¶ssories such as eyeglasses, sunglasses, hats,...", "https://github.com/NVlabs/ffhq-dataset", "Image", "Super Resolution", "Image_Generation,Image_Super-Resolution,Image_Inpainting", "-", "33", "-"];
    dataset[33] = ["ucf", "ucf", "UCF101 dataset is an extension of UCF50 and consi‚Ä¶ Human-object interactions, Playing musical i...", "https://www.crcv.ucf.edu/data/UCF101.php#Results_on_UCF101", "Video", "Action Recognition", "Temporal_Action_Localization,Action_Recognition,Action_Detection", "-", "1", "-"];
    dataset[34] = ["activitynet", "Activitynet", "The ActivityNet dataset contains 200 different ty‚Ä¶ms of both the number of activity categories ...", "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "Video", "Action Recognition", "Temporal_Action_Localization,Action_Recognition,Action_Classification", "-", "2", "-"];
    dataset[35] = ["ntu", "ntu", "-", "http://rose1.ntu.edu.sg/datasets/actionrecognition.asp", "Video", "Action Recognition", "-", "-", "3", "-"];
    dataset[36] = ["kinetics", "kinetics", "The Kinetics dataset is a large-scale, high-quali‚Ä¶clips for each action class. Each video clip ...", "https://arxiv.org/abs/1705.06950", "Video", "Action Recognition", "Temporal_Action_Localization,Video_Classification,Action_Recognition", "-", "4", "-"];
    dataset[37] = ["youtube_8m_segments_dataset", "YouTube-8M Segments Dataset", "The YouTube-8M dataset is a large scale video dat‚Ä¶set, and test set. In the training set, each ...", "http://research.google.com/youtube8m/index.html", "Video", "Classification", "Video_Classification,Video_Prediction", "8 million", "5", "-"];
    dataset[38] = ["davis_16", "davis 16", "DAVIS16 is a dataset for video object segmentatio‚Ä¶). Per-frame pixel-wise annotations are offered.", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "Video_Object_Segmentation,Video_Salient_Object_Detection,Unsupervised_Video_Object_Segmentation", "-", "6", "-"];
    dataset[39] = ["davis_17", "davis 17", "DAVIS17 is a dataset for video object segmentatio‚Ä¶ for training, 30 for validation, 60 for testing", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "Semantic_Segmentation,Video_Object_Segmentation,Referring_Expression_Segmentation", "-", "7", "-"];
    dataset[40] = ["davis_18", "davis 18", "-", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "-", "-", "8", "-"];
    dataset[41] = ["davis_19", "davis 19", "-", "https://davischallenge.org/index.html", "Video", "Object Segmentation", "-", "-", "9", "-"];
    dataset[42] = ["mot", "MOT", "-", "https://motchallenge.net/", "Video", "Object Tracking", "-", "-", "10", "-"];
    dataset[43] = ["vot", "vot", "-", "https://www.votchallenge.net/index.html", "Video", "Object Tracking", "-", "-", "11", "-"];
    dataset[44] = ["dexter", "dexter", "-", "http://archive.ics.uci.edu/ml//datasets/Dexter", "Text", "Classification", "-", "2600", "1", "-"];
    dataset[45] = ["ubuntu_dialogue", "ubuntu dialogue", "Ubuntu Dialogue Corpus (UDC) is a dataset contain‚Ä¶ilding dialogue managers based on neural lang...", "https://ubuntudialogue.org/", "Text", "Dialogue Generation", "Dialogue_Generation,Conversational_Response_Selection,Answer_Selection", "-", "2", "-"];
    dataset[46] = ["wmt19", "wmt19", "-", "http://www.statmt.org/wmt19/", "Text", "Machine Translation", "-", "-", "3", "-"];
    dataset[47] = ["wmt18", "wmt18", "WMT 2018 is a collection of datasets used in shar‚Ä¶on. &nbsp;&nbsp;&nbsp;The conference featured...", "http://www.statmt.org/wmt18/papers.html", "Text", "Machine Translation", "Machine_Translation", "-", "4", "-"];
    dataset[48] = ["wmt17", "wmt17", "-", "http://www.statmt.org/wmt17/results.html", "Text", "Machine Translation", "-", "-", "5", "-"];
    dataset[49] = ["wmt16", "wmt16", "WMT 2016 is a collection of datasets used in shar‚Ä¶red tasks: &nbsp;&nbsp;&nbsp;a news translati...", "http://www.statmt.org/wmt16/", "Text", "Machine Translation", "Machine_Translation,Unsupervised_Machine_Translation", "-", "6", "-"];
    dataset[50] = ["wmt15", "wmt15", "WMT 2015 is a collection of datasets used in shar‚Ä¶quality estimation task, &nbsp;an automatic p...", "http://www.statmt.org/wmt15/", "Text", "Machine Translation", "Machine_Translation", "-", "7", "-"];
    dataset[51] = ["wmt14", "wmt14", "WMT 2014 is a collection of datasets used in shar‚Ä¶ics task, &nbsp;a medical text translation task.", "http://www.statmt.org/wmt14/", "Text", "Machine Translation", "Machine_Translation,Unsupervised_Machine_Translation", "-", "8", "-"];
    dataset[52] = ["semeval-2016", "SemEval-2016", "-", "https://alt.qcri.org/semeval2016/index.php?id=tasks", "Text", "Word Sentiment", "-", "-", "9", "-"];
    dataset[53] = ["bfm", "BFM", "-", "https://faces.dmi.unibas.ch/bfm/?nav=1-0&amp;id=basel_face_model", "3-D Image", "3-D Estimation", "-", "-", "1", "-"];
    dataset[54] = ["pix3d", "Pix3D", "The Pix3D dataset is a large-scale benchmark of d‚Ä¶struction, retrieval, viewpoint estimation, etc.", "http://pix3d.csail.mit.edu/", "3-D Image", "Classification", "3D_Shape_Reconstruction,3D_Shape_Modeling,3D_Shape_Classification", "-", "2", "-"];
    dataset[55] = ["shrec", "shrec", "The SHREC dataset contains 14 dynamic gestures pe‚Ä¶ and 10 times by each participant in two way:...", "http://tosca.cs.technion.ac.il/book/shrec_robustness2010.html", "3-D Image", "Object Recognition", "Gesture_Recognition,Hand_Gesture_Recognition,Skeleton_Based_Action_Recognition", "-", "3", "-"];
    dataset[56] = ["shapenetcore", "shapenetCore", "-", "https://www.shapenet.org/", "3-D Image", "Semantic Segmentation", "-", "-", "4", "-"];
    dataset[57] = ["faust", "faust", "The FAUST dataset is a dataset of real 3D scans o‚Ä¶100 non-watertight meshes with 6,890 nodes each.", "http://faust.is.tue.mpg.de/", "3-D Image", "Semantic Segmentation", "Semantic_Segmentation,3D_Reconstruction,3D_Point_Cloud_Matching", "-", "5", "-"];
    dataset[58] = ["scape", "Scape", "-", "https://ai.stanford.edu/~drago/Projects/scape/scape.html", "3-D Image", "3-D Estimation", "-", "-", "6", "-"];
    dataset[59] = ["voxceleb", "VoxCeleb", "-", "http://www.robots.ox.ac.uk/~vgg/data/voxceleb/", "Sound", "Video Reconstruction", "-", "-", "1", "-"];

    var colosseum = [];
    colosseum[0] = ["Ï≤úÌïòÏ†úÏùº ÏñºÍµ¥ Ïù∏ÏãùÎåÄÌöå","2022.04.01","2022.05.01","celebA"];
    colosseum[1] = ["ÎèÑÎ°úÏúÑ Ïù∏ÏãùÏùò ÏµúÍ∞ïÏûê ÏÑ†Î∞ú ÎåÄÌöå","2022.06.01","2022.07.01","kitti"];
    colosseum[2] = ["Did you understand what I said?","2022.08.01","2022.09.01","wmt19"];
    
    //board.sort(date_descending); //ÎÇ†Ïßú ÎÇ¥Î¶ºÏ∞®Ïàú
    function date_descending(a, b) {
    var dateA = new Date(a[2]).getTime();
    var dateB = new Date(b[2]).getTime();
    return dateA < dateB ? 1 : -1;};
    
    function number_descending(a, b) { // ÎÇ¥Î¶ºÏ∞®Ïàú  
    var numA = (a[3]);
    var numB = (b[3]);  
    return numB - numA;}

    function dataset_number_descending(a, b) { //dataset Îç∞Ïù¥ÌÑ∞ÏÖã Ïà´Ïûê ÎÇ¥Î¶ºÏ∞®Ïàú  
    var numA = (a[8]);
    var numB = (b[8]);  
    return numB - numA;}

    function save_article(i) {//Í∏Ä Ï†úÎ™©ÏùÑ ÎàåÎ†ÄÏùÑÎïå Ïù¥ Í∏ÄÏùò board Ïù∏Îç±Ïä§Î•º 6-1forum-contentsÏóêÍ≤å ÎÑòÍ≤®Ï£ºÍ∏∞ ÏúÑÌï®
        window.localStorage.setItem("forum_value_1",i);
    }

    function make_list1(){//ÏôºÏ™Ω ÏïåÎ¶ºÌåê Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±
        board.sort(date_descending);
        apply_change();//Ï†ïÎ†¨ÌïòÍ≥† Ï†ïÎ†¨Ìïú boardÎ•º localstorageÏóê Îã§Ïãú Ï†ÄÏû•Ìï¥ÏÑú ÏµúÏã† ÏÉÅÌÉúÎ•º Î∞òÏòÅÌï® 
        var list1 = document.getElementById("list1");//DOM
        var length = dataset.length;
        var final_code="";
        for (var i = 2; i>=0; i--){
            var code="<li><a href=''>"+dataset[length-1-i][1]+" / "+dataset[length-1-i][4]+"</a></li>";
            final_code=final_code+code;
        }

        var date1 = new Date('2022.06.24');
        for (var i = 0; i<3; i++){
            var date2 = new Date(colosseum[i][1]);
            if(date2 > date1){
                var code="<li><a href=''>"+colosseum[i][0]+" / "+colosseum[i][3]+" / "+colosseum[i][1]+"~"+colosseum[i][2]+"</a></li>";
                final_code=final_code+code;
            }
        }
        list1.innerHTML = final_code;//innerHTMLÎ°ú divÏóê HTMLÏΩîÎìú ÏÇΩÏûÖ
    }
    

    function make_list2(){//Ïò§Î•∏Ï™Ω ÏïåÎ¶ºÌåê Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±
        board.sort(number_descending);
        apply_change();
        dataset.sort(dataset_number_descending);
        var list2 = document.getElementById("list2");
        var final_code="";
        for (var i = 0; i<3; i++){
            var code="<li onclick='save_article("+i+")'><a href='6-1forum-contents.html'>"+board[i][0]+" / "+board[i][2]+"</a></li>";
            final_code=final_code+code;
        }
        for (var i = 0; i<3; i++){
            var code="<li><a href=''>"+colosseum[i][0]+" / "+colosseum[i][3]+" / "+colosseum[i][1]+"~"+colosseum[i][2]+"</a></li>";
            final_code=final_code+code;
        }
        for (var i = 0; i<3; i++){
            var code="<li><a href=''>"+dataset[i][1]+" / "+dataset[i][4]+"</a></li>";
            final_code=final_code+code;
        }
        list2.innerHTML = final_code;
    }
    

    function apply_change(){//boardÎ•º localstorageÏóê Îã§Ïãú Ï†ÄÏû•Ìï¥ÏÑú ÏµúÏã† ÏÉÅÌÉúÎ•º Î∞òÏòÅÌï® 
        window.localStorage.setItem("board",JSON.stringify(board));//localstorageÏóê Î∞∞Ïó¥ÏùÑ Ï†ÄÏû•ÌïòÎäî Î∞©Î≤ï
    }

    config();
    var output = localStorage.getItem("board");
    var board = JSON.parse(output);//localstorageÏóê Ï†ÄÏû•Îêú Î∞∞Ïó¥ÏùÑ Ïó¨Í∏∞ÏÑú ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ Î∂àÎü¨Ïò¥
    make_list1();
    make_list2();
    </script>

</html>